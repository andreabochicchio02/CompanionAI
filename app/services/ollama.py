import app.services.utils as utils

import requests
import json

URL = "http://localhost:11434/api/generate"

def query_ollama_no_stream(prompt, model):
    """
    Send a prompt to the Ollama API and get the full response without streaming.
    Returns the complete generated text as a string.
    """

    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False  # Disable streaming to get the entire response at once
    }

    try:
        utils.append_log_complete(f"SENT:\n {prompt}\n")
        with requests.post(URL, json=payload, stream=False) as response:
            response.raise_for_status()  # Raise an exception for HTTP errors
            data = response.json()  # Parse JSON response
            response_text = data.get("response", "").strip()
            utils.append_log_complete(f"RECEIVED:\n {response_text}\n")
            return response_text  # Return the text generated by the model
    except requests.exceptions.ConnectionError:
        utils.append_log("Error: Cannot connect to Ollama. Is it running?")
    except Exception as e:
        utils.append_log(f"Unexpected error: {e}")
    else:
        utils.append_log("Response generated and sent successfully (stream=False)")

def query_ollama_streaming(prompt, model):
    """
    Generator function that streams response chunks from the Ollama API.
    Yields each piece of generated text as it arrives.
    """
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": True  # Enable streaming to receive partial responses
    }

    try:
        utils.append_log_complete(f"SENT:\n {prompt}\n")
        accumulated_response = ""
        with requests.post(URL, json=payload, stream=True) as response:
            response.raise_for_status()
            for line in response.iter_lines():  # Iterate over each line of the streamed response
                if line:
                    chunk = json.loads(line)  # Parse each JSON chunk
                    if 'response' in chunk:
                        response_text = chunk['response']
                        accumulated_response += response_text 
                        yield response_text  # Yield the generated text piece
                    if chunk.get('done', False):  # Stop if the 'done' flag is True
                        break

        utils.append_log_complete(f"RECEIVED:\n {accumulated_response}\n")
    except requests.exceptions.ConnectionError:
        utils.append_log("Error: Cannot connect to Ollama. Is it running?")
    except Exception as e:
        utils.append_log(f"Unexpected error: {e}")
    else:
        utils.append_log("Response generated and sent successfully (stream=True)")

def preload_model(model_name):
    """
    Preload the specified model into memory by sending an empty prompt.
    Useful for reducing the delay before the first real request.
    """
    prompt = ""
    payload = {
        "model": model_name,
        "prompt": prompt,
        "stream": False
    }

    try:
        with requests.post(URL, json=payload, stream=True) as response:
            response.raise_for_status()  # Check for HTTP errors
    except Exception as e:
        utils.append_log(f"Failed to preload model '{model_name}': {e}")
    else:
        utils.append_log(f"Model {model_name} preload correctly")